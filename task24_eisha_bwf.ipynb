{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Main points of the task are:\n",
        "1. Data cleaning: Before starting any analysis, it's important to clean and preprocess the data. This may involve handling missing values, dealing with outliers, and removing irrelevant features.\n",
        "\n",
        "2. Normalization and scaling: Neural networks typically require inputs to be on a similar scale, so it's often necessary to normalize or scale the data before feeding it into the network. Common methods include min-max scaling, z-score normalization, and log scaling.\n",
        "\n",
        "3. One-hot encoding: Categorical variables can't be directly used as input for neural networks, so they need to be converted into a numerical format. One-hot encoding is a common technique for doing this, which creates a binary column for each category in the variable.\n",
        "\n",
        "4. Feature engineering: In some cases, it may be helpful to engineer new features that better capture the underlying structure of the data. For example, if working with image data, we may want to extract features like edges or textures.\n",
        "\n",
        "5. Data augmentation: Data augmentation involves creating new training data by transforming the existing data in various ways. For example, we might flip images horizontally or add noise to audio recordings.\n",
        "\n",
        "Example is:"
      ],
      "metadata": {
        "id": "15979sP0n8No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbND1dIon8XB",
        "outputId": "0cdd893e-33a5-4b3d-babb-fe495f52c53f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 160)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.6620 - acc: 0.6414 - val_loss: 0.6054 - val_acc: 0.6956\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5315 - acc: 0.7511 - val_loss: 0.5233 - val_acc: 0.7270\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4589 - acc: 0.7881 - val_loss: 0.5002 - val_acc: 0.7432\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.4234 - acc: 0.8077 - val_loss: 0.4959 - val_acc: 0.7508\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3988 - acc: 0.8199 - val_loss: 0.4951 - val_acc: 0.7578\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3787 - acc: 0.8310 - val_loss: 0.4999 - val_acc: 0.7564\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3600 - acc: 0.8426 - val_loss: 0.5039 - val_acc: 0.7548\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3428 - acc: 0.8528 - val_loss: 0.5108 - val_acc: 0.7538\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.3251 - acc: 0.8623 - val_loss: 0.5144 - val_acc: 0.7552\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3084 - acc: 0.8719 - val_loss: 0.5219 - val_acc: 0.7540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are using the IMDb movie review sentiment classification dataset from Keras. We first load the data and set the maximum number of features to 10,000 and the maximum sequence length to 20. Then, we preprocess the data by padding the sequences with zeros to ensure they are all the same length.\n",
        "\n",
        "We then build a simple neural network model with an embedding layer, a flatten layer, and a dense layer with a sigmoid activation function. We compile the model using binary cross-entropy loss and accuracy as the evaluation metric.\n",
        "\n",
        "Finally, we train the model for 10 epochs with a batch size of 32 and a validation split of 0.2. The history object is used to plot the loss and accuracy curves."
      ],
      "metadata": {
        "id": "TGm6vGqfrN5O"
      }
    }
  ]
}